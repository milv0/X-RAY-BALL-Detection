{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1774d805-cda0-43fc-bbeb-48df6b31e57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.1\n",
      "CUDA is available on this machine.\n",
      "Allocated GPU memory: 0.00 GB\n",
      "Reserved GPU memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    allocated_memory = torch.cuda.memory_allocated() / (1024 ** 3)  # 기가바이트 단위로 변환\n",
    "    reserved_memory = torch.cuda.max_memory_reserved() / (1024 ** 3)\n",
    "    print(\"CUDA is available on this machine.\")\n",
    "\n",
    "    print(f\"Allocated GPU memory: {allocated_memory:.2f} GB\")\n",
    "    print(f\"Reserved GPU memory: {reserved_memory:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this machine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e49b1774-5d81-4f95-91a4-d854ab93c517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from preprocessing.single_image_enhance_tflite import zeroDCE\n",
    "from ultralytics import YOLO\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "deep_available = False\n",
    "is_timekeeping = True\n",
    "visualable = False\n",
    "brightiable = True\n",
    "savenable = True\n",
    "\n",
    "\n",
    "''' 반도체 이미지 폴더 '''\n",
    "# path = os.path.join(current_directory, '...')\n",
    "# path = os.path.join(current_directory, 'input_imgs/demo/')\n",
    "# path = os.path.join(current_directory, 'test_demo/')\n",
    "# path = os.path.join(current_directory, 'input_imgs/OK/')\n",
    "# path = os.path.join(current_directory, 'test_demo/')\n",
    "path = os.path.join(current_directory, '3OK/')\n",
    "\n",
    "\n",
    "''' 결과 저장 폴더 '''\n",
    "# save_path = os.path.join(current_directory, '...')\n",
    "# save_path = os.path.join(current_directory, 'saved_imgs/demo')\n",
    "# save_path = os.path.join(current_directory, 'saved_imgs')\n",
    "# save_path = os.path.join(current_directory, 'saved_imgs/OK')\n",
    "# save_path = os.path.join(current_directory, 'saved_imgs/.test')\n",
    "save_path = os.path.join(current_directory, 'saved_imgs/3OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b5a4210-87ab-42e4-9114-641491d98f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO 모델 \n",
    "YOLOv8_small_seg = YOLO(\"yolov8s-seg.pt\")\n",
    "\n",
    "def xyxy2xy(xyxy):\n",
    "    x = xyxy[:,0] + xyxy[:,2]    \n",
    "    y = xyxy[:,1] + xyxy[:,3]\n",
    "    return torch.cat((y.unsqueeze(1), x.unsqueeze(1)), dim=1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edac6217-4ad3-4ea2-b4bf-6250837310bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter2D(img):\n",
    "    # 커널 마스크 생성\n",
    "    kernel = np.ones((3,3), dtype=np.float64) / 9. \n",
    "    \n",
    "    return cv2.filter2D(img, -1, kernel)\n",
    "\n",
    "def canny(img):\n",
    "    # 채널 단일화\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    '''\n",
    "    img, threshold1, threshold2\n",
    "    img : 입력 이미지, 보통 그레이스케일 이미지 사용\n",
    "    threshold1 : 하위 임계값, 이 값보다 작을 경우 무시됨\n",
    "    threshold2 : 상위 임계값, 이 값보다 클 경우 확실한 edge로 간주\n",
    "    '''\n",
    "    edge = cv2.Canny(gray, 28, 37)\n",
    "\n",
    "    # 빈 BGR 이미지 생성\n",
    "    bgr_img = cv2.cvtColor(edge, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    # 엣지 픽셀의 위치에 흰색[255, 255, 255]으로 표시\n",
    "    bgr_img[edge != 0] = [255, 255, 255]\n",
    "\n",
    "    return edge, bgr_img\n",
    "\n",
    "def merge(origin, modified):\n",
    "    background = Image.fromarray(origin)\n",
    "    foreground = Image.fromarray(modified)\n",
    "    background.paste(foreground, (0,0), foreground)\n",
    "\n",
    "    return np.array(background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae115194-a520-4842-b2ad-193d45a74049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def natural_sort_key(string):\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split('(\\d+)', string)]\n",
    "\n",
    "class Image_Loader(Dataset):\n",
    "    def __init__(self, dir_path, brighter, transformer=None):\n",
    "        \"\"\"\n",
    "        파라미터 종류:\n",
    "        이미지 폴더 경로, 시각화 여부, 흑백대비 여부, 딥러닝 여부\n",
    "        \"\"\"\n",
    "        imgs_paths = np.array([dir_path + img_path for img_path in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, img_path))])\n",
    "        self.imgs_paths = np.array(sorted(imgs_paths, key=natural_sort_key))\n",
    "        self.imgs_paths = self.imgs_paths[0:]\n",
    "        \n",
    "        self.brighter = brighter\n",
    "  \n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_paths)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        canny_img = None\n",
    "        merge_img = None\n",
    "        \n",
    "        img_path = self.imgs_paths[index]\n",
    "        img = cv2.imread(img_path)\n",
    "        original_filename = os.path.basename(img_path)  # 원본 이미지 이름 추출\n",
    "\n",
    "\n",
    "        if self.brighter:\n",
    "            img = zeroDCE(img_path=img_path).numpy()\n",
    "\n",
    "            img = filter2D(img)\n",
    "\n",
    "            canny_img, canny_with_bw = canny(img)\n",
    "            merge_img = merge(img, canny_img)\n",
    "\n",
    "            img = merge_img\n",
    "        \n",
    "        if self.transformer:\n",
    "            img = torch.from_numpy(img)\n",
    "            \"\"\"\n",
    "            딥러닝 시,\n",
    "            np.numpy -> torch.tensor로 변환 + 관련 추가 전처리 작성\n",
    "            \"\"\"\n",
    "            img = img.to(device)\n",
    "\n",
    "        return img, original_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87a5feab-1d22-4ecd-8fd9-f7b6590257bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "12시 방향 range\n",
    "\"\"\"\n",
    "def range_12(img, points, roi, AF, masks, normality):\n",
    "    tmp = normality\n",
    "    normality = 1\n",
    "    \n",
    "    \"\"\"\n",
    "    ㄷ자 mid 방향 처리\n",
    "    \"\"\"\n",
    "    x_mid = np.int16(np.around(roi[1] + 220))\n",
    "    y_mid = np.int16(np.around(roi[0] - 1264))\n",
    "    w_mid = np.int16(np.around(roi[1] + 550))\n",
    "    h_mid = np.int16(np.around(roi[0] - 750))\n",
    "\n",
    "    x_AF = np.int16(np.around(roi[1] + 220))\n",
    "    y_AF = np.int16(np.around(roi[0] - 1200))\n",
    "    w_AF = np.int16(np.around(roi[1] + 310))\n",
    "    h_AF = np.int16(np.around(roi[0] - 937))\n",
    "    \n",
    "    # 12시 방향 범위 내의 점들 선택\n",
    "    patch_idx = np.where(\n",
    "        (points[:, 0] > x_mid) &\\\n",
    "        (points[:, 0] < w_mid) &\\\n",
    "        (points[:, 1] > y_mid) &\\\n",
    "        (points[:, 1] < h_mid)\n",
    "    )\n",
    "    patch_mid = points[patch_idx[0]]\n",
    "\n",
    "    patch_idx = np.where(\n",
    "        (patch_mid[:, 0] <= x_AF) |\\\n",
    "        (patch_mid[:, 0] >= w_AF) |\\\n",
    "        (patch_mid[:, 1] <= y_AF) |\\\n",
    "        (patch_mid[:, 1] >= h_AF)\n",
    "    )\n",
    "    patch_mid = patch_mid[patch_idx[0]]\n",
    "\n",
    "    # 색상 조건에 맞는 점들만 선택\n",
    "    patch_mid = [circle for circle in patch_mid if all(color <= 94 for color in img[circle[1], circle[0]])]\n",
    "\n",
    "    # AF 세그멘테이션 탐지 및 처리\n",
    "    if ('-AF_2,' in AF) or ('-AF_2s,' in AF):\n",
    "        normality = range_af_2(AF, normality, patch_mid, masks)\n",
    "    else:\n",
    "        normality = 0\n",
    "\n",
    "    # 12시 방향 결과 시각화\n",
    "    visualize_result(img, patch_mid, x_mid, y_mid, w_mid, h_mid, normality)\n",
    "    visualize_result(img, patch_mid, x_AF, y_AF, w_AF, h_AF, normality) \n",
    "  \n",
    "    return np.min([normality, tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b55107f8-bebc-44b9-901b-28fcc70deef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "9시 방향 range\n",
    "\"\"\"\n",
    "def range_9(img, points, roi, AF, masks, normality):\n",
    "    tmp = normality\n",
    "    normality = 1   \n",
    "    \n",
    "    \"\"\"\n",
    "    Right 부분 처리\n",
    "    \"\"\"\n",
    "    x_right = np.int16(np.around(roi[1] - 820))\n",
    "    y_right = np.int16(np.around(roi[0] - 405)) # y < 402\n",
    "    w_right = np.int16(np.around(roi[1] - 280))\n",
    "    h_right = np.int16(np.around(roi[0] - 174))\n",
    "\n",
    "    x_AF = np.int16(np.around(roi[1] - 640))\n",
    "    y_AF = np.int16(np.around(roi[0] - 377))\n",
    "    w_AF = np.int16(np.around(roi[1] - 560))\n",
    "    h_AF = np.int16(np.around(roi[0] - 267))    \n",
    "    \n",
    "    patch_idx = np.where(\n",
    "        (points[:, 0] > x_right) &\\\n",
    "        (points[:, 0] < w_right) &\\\n",
    "        (points[:, 1] > y_right) &\\\n",
    "        (points[:, 1] < h_right)\n",
    "    )\n",
    "    patch_right = points[patch_idx[0]]\n",
    "\n",
    "    patch_idx = np.where(\n",
    "        (patch_right[:, 0] <= x_AF) |\\\n",
    "        (patch_right[:, 0] >= w_AF) |\\\n",
    "        (patch_right[:, 1] <= y_AF) |\\\n",
    "        (patch_right[:, 1] >= h_AF)\n",
    "    )\n",
    "    patch_right = patch_right[patch_idx[0]]    \n",
    "       \n",
    "    patch_right = [circle for circle in patch_right if all(color < 83 for color in img[circle[1], circle[0]])]\n",
    "    if len(patch_right) != 2:\n",
    "        normality = 0\n",
    "    else:\n",
    "        #if ('-AF_1,' in AF) or ('-AF_1s,' in AF):\n",
    "        if ('-AF_1,' in AF) or ('-AF_1s,' in AF):\n",
    "            normality = range_af_1(AF, normality, masks)\n",
    "        else:\n",
    "            normality = 0\n",
    "\n",
    "    # 9시 방향 right 결과 시각화\n",
    "    visualize_result(img, patch_right, x_right, y_right, w_right, h_right, normality)\n",
    "    visualize_result(img, patch_right, x_AF, y_AF, w_AF, h_AF, normality)\n",
    "\n",
    " \n",
    "    return np.min([normality, tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ae0a04b-9fac-43d8-a37b-ca24b56c867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6시 방향 range\n",
    "\"\"\"\n",
    "def range_6(img, points, roi, normality):\n",
    "    tmp = normality\n",
    "    normality = 1   \n",
    "    \n",
    "    x = np.int16(np.around(roi[1] + 330))\n",
    "    y = np.int16(np.around(roi[0] + 350))\n",
    "    w = np.int16(np.around(roi[1] + 645)) # 602 < w\n",
    "    h = np.int16(np.around(roi[0] + 606))\n",
    "    \n",
    "    x_2 = np.int16(np.around(roi[1] + 624))\n",
    "    y_2 = np.int16(np.around(roi[0] + 350))\n",
    "    w_2 = np.int16(np.around(roi[1] + 645))\n",
    "    h_2 = np.int16(np.around(roi[0] + 425))\n",
    "\n",
    "    x_3 = np.int16(np.around(roi[1] + 631))\n",
    "    y_3 = np.int16(np.around(roi[0] + 470))\n",
    "    w_3 = np.int16(np.around(roi[1] + 633))\n",
    "    h_3 = np.int16(np.around(roi[0] + 472))\n",
    "    \n",
    "    x_AF = np.int16(np.around(roi[1] + 330))\n",
    "    y_AF = np.int16(np.around(roi[0] + 350))\n",
    "    w_AF = np.int16(np.around(roi[1] + 360))\n",
    "    h_AF = np.int16(np.around(roi[0] + 400))      \n",
    "    \n",
    "    patch_idx = np.where(\n",
    "        (points[:, 0] > x) &\\\n",
    "        (points[:, 0] < w) &\\\n",
    "        (points[:, 1] > y) &\\\n",
    "        (points[:, 1] < h)\n",
    "    )\n",
    "    patch3 = points[patch_idx[0]]\n",
    "\n",
    "    patch_idx = np.where(\n",
    "        (patch3[:, 0] <= x_2) |\\\n",
    "        (patch3[:, 0] >= w_2) |\\\n",
    "        (patch3[:, 1] <= y_2) |\\\n",
    "        (patch3[:, 1] >= h_2)\n",
    "    )\n",
    "    patch3 = patch3[patch_idx[0]]  \n",
    "    \n",
    "    patch_idx = np.where(\n",
    "        (patch3[:, 0] <= x_3) |\\\n",
    "        (patch3[:, 0] >= w_3) |\\\n",
    "        (patch3[:, 1] <= y_3) |\\\n",
    "        (patch3[:, 1] >= h_3)\n",
    "    )\n",
    "    patch3 = patch3[patch_idx[0]]  \n",
    "    \n",
    "    patch_idx = np.where(\n",
    "        (patch3[:, 0] <= x_AF) |\\\n",
    "        (patch3[:, 0] >= w_AF) |\\\n",
    "        (patch3[:, 1] <= y_AF) |\\\n",
    "        (patch3[:, 1] >= h_AF)\n",
    "    )\n",
    "    patch3 = patch3[patch_idx[0]]    \n",
    "    \n",
    "    patch3 = [circle for circle in patch3 if all(color < 100 for color in img[circle[1], circle[0]])]\n",
    "\n",
    "    # 유클리드 거리가 120 미만인 점들만 남기기\n",
    "    patch3 = np.array(patch3)\n",
    "    if len(patch3) > 1:\n",
    "        dists = distance.cdist(patch3, patch3, 'euclidean')\n",
    "        close_points = np.any((dists < 91) & (dists != 0), axis=1)\n",
    "        patch3 = patch3[close_points]\n",
    "\n",
    "    if len(patch3) != 2:\n",
    "        normality = 0\n",
    "    \n",
    "    visualize_result(img, patch3, x, y, w, h, normality)\n",
    "    visualize_result(img, patch3, x_AF, y_AF, w_AF, h_AF, normality)\n",
    "    visualize_result(img, patch3, x_2, y_2, w_2, h_2, normality)\n",
    "    visualize_result(img, patch3, x_3, y_3, w_3, h_3, normality)    \n",
    "\n",
    "    return np.min([normality, tmp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22e215d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3시 방향 range\n",
    "\"\"\"\n",
    "def range_3(img, points, roi, normality):\n",
    "    tmp = normality\n",
    "    normality = 1   \n",
    "    \n",
    "    x = np.int16(np.around(roi[1] + 1100))\n",
    "    y = np.int16(np.around(roi[0] - 450))\n",
    "    w = np.int16(np.around(roi[1] + 1500))\n",
    "    h = np.int16(np.around(roi[0] - 151)) #150 <=h\n",
    "    \n",
    "    x_AF = np.int16(np.around(roi[1] + 1207))\n",
    "    y_AF = np.int16(np.around(roi[0] - 156))\n",
    "    w_AF = np.int16(np.around(roi[1] + 1209))\n",
    "    h_AF = np.int16(np.around(roi[0] - 151))  \n",
    "    \n",
    "    patch_idx = np.where(\n",
    "        (points[:, 0] > x) &\\\n",
    "        (points[:, 0] < w) &\\\n",
    "        (points[:, 1] > y) &\\\n",
    "        (points[:, 1] < h)\n",
    "    )\n",
    "    patch4 = points[patch_idx[0]]\n",
    "    \n",
    "    patch_idx = np.where(\n",
    "        (patch4[:, 0] <= x_AF) |\\\n",
    "        (patch4[:, 0] >= w_AF) |\\\n",
    "        (patch4[:, 1] <= y_AF) |\\\n",
    "        (patch4[:, 1] >= h_AF)\n",
    "    )\n",
    "    patch4 = patch4[patch_idx[0]] \n",
    "    \n",
    "    patch4 = [circle for circle in patch4 if all(color < 92 for color in img[circle[1], circle[0]])] # 92\n",
    "\n",
    "    # 유클리드 거리가 120 미만인 점들만 남기기\n",
    "    patch4 = np.array(patch4)\n",
    "    if len(patch4) > 1:\n",
    "        dists = distance.cdist(patch4, patch4, 'euclidean')\n",
    "        close_points = np.any((dists < 140) & (dists != 0), axis=1)\n",
    "        patch4 = patch4[close_points]\n",
    "\n",
    "    if len(patch4) != 2:\n",
    "        normality = 0\n",
    "        \n",
    "    visualize_result(img, patch4, x, y, w, h, normality)\n",
    "    visualize_result(img, patch4, x_AF, y_AF, w_AF, h_AF, normality)\n",
    "\n",
    "    return np.min([normality, tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d67b52db-7da2-482a-9de7-9a899d10a0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "9시 방향 left range에 사용됨\n",
    "\"\"\"\n",
    "def range_af_1(AF, normality,patch, masks):\n",
    "    Vertical_Length1 = []\n",
    "    low_threshold = 52\n",
    "    high_threshold = 116\n",
    "    mask_array = masks.data.cpu().numpy()\n",
    "    for i, mask in enumerate(mask_array):\n",
    "        if len(mask.shape) > 2:\n",
    "            mask = mask.squeeze()\n",
    "        vertical_length1 = np.sum(mask, axis=0).max()\n",
    "        Vertical_Length1.append(vertical_length1)\n",
    "    is_seg = True\n",
    "    for a in Vertical_Length1:\n",
    "        if a < low_threshold:\n",
    "            is_seg = False\n",
    "            \n",
    "        elif a > high_threshold:\n",
    "            is_seg = False\n",
    "    idx = 0 \n",
    "    if ('-AF_1,' in AF):\n",
    "        idx = AF.index('-AF_1,') - 1\n",
    "    elif ('-AF_1s,' in AF):\n",
    "        idx = AF.index('-AF_1s,') - 1\n",
    "\n",
    "    if AF[idx] != '1':\n",
    "        AF[idx] = '1'\n",
    "\n",
    "    if (int(AF[idx]) * 2) != 2:\n",
    "        normality = 0\n",
    "\n",
    "    if not is_seg:\n",
    "        normality = 0  \n",
    "    \n",
    "    return normality\n",
    "\n",
    "\"\"\"\n",
    "12시 방향 range에 사용됨\n",
    "\"\"\"\n",
    "def range_af_2(AF, normality, patch1, masks):\n",
    "    Vertical_Length = []\n",
    "    threshold = 92\n",
    "    mask_array = masks.data.cpu().numpy()\n",
    "    for i, mask in enumerate(mask_array):\n",
    "        if len(mask.shape) > 2:\n",
    "            mask = mask.squeeze()\n",
    "        vertical_length = np.sum(mask, axis=0).max()\n",
    "        Vertical_Length.append(vertical_length)\n",
    "\n",
    "    is_seg = False\n",
    "    for a in Vertical_Length:\n",
    "        if a > threshold:\n",
    "            is_seg = True\n",
    "            idx = 0\n",
    "            if ('-AF_2,' in AF):\n",
    "                idx = AF.index('-AF_2,') - 1\n",
    "            elif ('-AF_2s,' in AF):\n",
    "                idx = AF.index('-AF_2s,') - 1\n",
    "\n",
    "            if AF[idx] != '1':\n",
    "                AF[idx] = '1'\n",
    "\n",
    "            if (len(patch1) + int(AF[idx]) * 3) != 5:\n",
    "                normality = 0\n",
    "                break\n",
    "\n",
    "    if not is_seg:\n",
    "        normality = 0\n",
    "\n",
    "    return normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c91a40ca-25da-41a1-909e-5a0a306334ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "이미지 시각화 -> 원탐지 탐지 x이거나 roi 이상한 것들만 시각화하는 코드로 변경 필요\n",
    "\"\"\"\n",
    "def visualize_result(img, patch, x, y, w, h, normality):\n",
    "    for itr in patch:\n",
    "        cv2.circle(img, (itr[0], itr[1]), 1, (0, 100, 100), 3)\n",
    "        cv2.circle(img, (itr[0], itr[1]), itr[2], (255, 0, 255), 2)\n",
    "        cv2.putText(img, f'r={itr[2]}', (itr[0] - itr[2], itr[1] + itr[2]), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2)\n",
    "        # cv2.putText(img, f'x=({itr[0]}, {itr[1]})', (itr[0], itr[1]), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1) # OIS 중심 좌표\n",
    "\n",
    "    if normality == 1:\n",
    "        line_color = (0, 255, 0)  \n",
    "        line_thickness = 2\n",
    "    else:\n",
    "        line_color = (0, 0, 255) \n",
    "        line_thickness = 2\n",
    "    \n",
    "    cv2.line(img, (x, y), (x, h), line_color, line_thickness)\n",
    "    cv2.line(img, (x, y), (w, y), line_color, line_thickness)\n",
    "    cv2.line(img, (w, y), (w, h), line_color, line_thickness)\n",
    "    cv2.line(img, (x, h), (w, h), line_color, line_thickness)\n",
    "\n",
    "''' 메모리 사용량 줄이기 '''\n",
    "def range_all(img, points, roi, AF, masks, normality):\n",
    "    normality = range_12(img, points, roi, AF, masks, normality)\n",
    "    normality = range_9(img, points, roi, AF, masks, normality)\n",
    "    normality = range_6(img, points, roi, normality)\n",
    "    normality = range_3(img, points, roi, normality)\n",
    "    \n",
    "    del points\n",
    "    return normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9b7ea6a-af6a-4f7b-a9b7-009ec2abdbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_zero_count = 0\n",
    "\n",
    "def zoom(img, masks, target_distance=1200, bg_color=(255, 255, 255)):\n",
    "    global distance_zero_count\n",
    "\n",
    "    def adjust_image(img, masks, target_distance, bg_color):\n",
    "        global distance_zero_count\n",
    "        \n",
    "        def euclidean_distance(point1, point2):\n",
    "            return math.sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)\n",
    "        \n",
    "        if not (masks and hasattr(masks, 'xy')):\n",
    "            distance_zero_count += 1\n",
    "            img_center = (img.shape[1] // 2, img.shape[0] // 2)\n",
    "            return img, img_center, 0\n",
    "\n",
    "        centers = [(mask[:, 0].mean(), mask[:, 1].mean()) for mask in masks.xy if len(mask) > 1]\n",
    "        img_center = (img.shape[1] // 2, img.shape[0] // 2)\n",
    "\n",
    "        if len(centers) < 2:\n",
    "            distance_zero_count += 1\n",
    "            return img, img_center, 0\n",
    "\n",
    "        distances_from_center = sorted([(math.sqrt((img_center[0] - x) ** 2 + (img_center[1] - y) ** 2), (x, y)) for x, y in centers])\n",
    "\n",
    "        filtered_centers = []\n",
    "        for i in range(len(distances_from_center)):\n",
    "            for j in range(i + 1, len(distances_from_center)):\n",
    "                distance_between_centers = euclidean_distance(distances_from_center[i][1], distances_from_center[j][1])\n",
    "                if distance_between_centers > 200:\n",
    "                    filtered_centers.append(distances_from_center[i][1])\n",
    "                    filtered_centers.append(distances_from_center[j][1])\n",
    "                    break\n",
    "            if len(filtered_centers) >= 2:\n",
    "                break\n",
    "\n",
    "        if len(filtered_centers) < 2:\n",
    "            distance_zero_count += 1\n",
    "            return img, img_center, 0\n",
    "\n",
    "        closest_centers = filtered_centers[:2]\n",
    "        distance = euclidean_distance(*closest_centers)\n",
    "        higher_point, lower_point = max(closest_centers, key=lambda p: p[1]), min(closest_centers, key=lambda p: p[1])\n",
    "        intersection_point = (int(lower_point[0]), int(higher_point[1]))\n",
    "        # cv2.circle(img, intersection_point, 10, (0, 255, 0), 20)\n",
    "\n",
    "        dx, dy = img_center[0] - intersection_point[0], img_center[1] - intersection_point[1]\n",
    "        moved_img = cv2.warpAffine(img, np.float32([[1, 0, dx], [0, 1, dy]]), (img.shape[1], img.shape[0]), borderMode=cv2.BORDER_CONSTANT, borderValue=bg_color)\n",
    "\n",
    "        if distance == 0:\n",
    "            return moved_img, img_center, 0\n",
    "        \n",
    "        scale_factor = target_distance / distance\n",
    "        new_size = (int(moved_img.shape[1] * scale_factor), int(moved_img.shape[0] * scale_factor))\n",
    "        resized_img = cv2.resize(moved_img, new_size, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        if scale_factor > 1:  # 확대\n",
    "            x_offset, y_offset = (resized_img.shape[1] - moved_img.shape[1]) // 2, (resized_img.shape[0] - moved_img.shape[0]) // 2\n",
    "            cropped_img = resized_img[y_offset:y_offset + moved_img.shape[0], x_offset:x_offset + moved_img.shape[1]]\n",
    "            return cropped_img, intersection_point, distance\n",
    "        else:  # 축소\n",
    "            canvas = np.full_like(moved_img, bg_color, dtype=np.uint8)\n",
    "            x_offset, y_offset = (moved_img.shape[1] - resized_img.shape[1]) // 2, (moved_img.shape[0] - resized_img.shape[0]) // 2\n",
    "            canvas[y_offset:y_offset + resized_img.shape[0], x_offset:x_offset + resized_img.shape[1]] = resized_img\n",
    "            return canvas, intersection_point, distance\n",
    "\n",
    "    adjusted_img, _, _ = adjust_image(img, masks, target_distance, bg_color)\n",
    "    return adjusted_img\n",
    "\n",
    "def get_distance_zero_count():\n",
    "    global distance_zero_count\n",
    "    return distance_zero_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "77372f2c-62c4-47dc-b2ef-f2fe55f6a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(img, save_info=False, visualizer=False):\n",
    "    torch.cuda.empty_cache()\n",
    "    [classes, img, masks] = YOLOv8_small_seg.predict(source=img, save=True, show_boxes=False, show_labels=False, show_conf=False, imgsz=1312, conf=0.6, retina_masks=False)\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    normality = 1\n",
    "    roi = None\n",
    "    if masks is None:\n",
    "        normality = 0\n",
    "    else:\n",
    "        try:\n",
    "            centers = [(mask[:, 0].mean(), mask[:, 1].mean()) for mask in masks.xy if len(mask) > 0 and len(mask[0]) > 1]\n",
    "            img_center = (img.shape[1] // 2, img.shape[0] // 2)\n",
    "\n",
    "            distances_from_center = sorted([(math.sqrt((img_center[0] - x) ** 2 + (img_center[1] - y) ** 2), (x, y)) for x, y in centers])\n",
    "\n",
    "            filtered_centers = []\n",
    "            for i in range(len(distances_from_center)):\n",
    "                for j in range(i + 1, len(distances_from_center)):\n",
    "                    distance_between_centers = math.sqrt((distances_from_center[i][1][0] - distances_from_center[j][1][0]) ** 2 + (distances_from_center[i][1][1] - distances_from_center[j][1][1]) ** 2)\n",
    "                    if distance_between_centers > 200:\n",
    "                        filtered_centers.append(distances_from_center[i][1])\n",
    "                        filtered_centers.append(distances_from_center[j][1])\n",
    "                        break\n",
    "                if len(filtered_centers) >= 2:\n",
    "                    break\n",
    "\n",
    "            closest_centers = filtered_centers[:2]\n",
    "\n",
    "            if len(closest_centers) > 1:\n",
    "                higher_point = max(closest_centers, key=lambda p: p[1])\n",
    "                lower_point = min(closest_centers, key=lambda p: p[1])\n",
    "                intersection_point = (lower_point[0], higher_point[1])\n",
    "\n",
    "                roi = (int(intersection_point[0]), int(intersection_point[1]))\n",
    "                # cv2.circle(img, roi, 10, (0, 0, 255), 20)\n",
    "\n",
    "        except IndexError:\n",
    "            roi = None\n",
    "\n",
    "    info = detection(img, method=Hough_Transform, brighted=brightiable, DL=deep_available)\n",
    "    if info is not None and info.size > 0:\n",
    "        info = np.uint16(np.around(info))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    if roi == None:\n",
    "        normality = 0\n",
    "    else:\n",
    "        points = np.array(info[0][:, :3])\n",
    "        normality = range_all(img, points, roi, classes, masks, normality)\n",
    "\n",
    "    if visualizer:\n",
    "        plt.figure(figsize=(16, 12))\n",
    "        plt.imshow(img)\n",
    "        plt.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "        plt.show()\n",
    "\n",
    "        if normality == 0:\n",
    "            print('불량')\n",
    "        else:\n",
    "            print('정상')\n",
    "\n",
    "    if save_info:\n",
    "        if (int(normality) + int(not 'NG' in path)) != 1:\n",
    "            if not os.path.exists(save_path + '/True/'):\n",
    "                os.makedirs(save_path + '/True/')\n",
    "            cv2.imwrite(os.path.join(save_path + '/True/', original_filename), img)\n",
    "\n",
    "        else:\n",
    "            if not os.path.exists(save_path + '/Fake/'):\n",
    "                os.makedirs(save_path + '/Fake/')\n",
    "            cv2.imwrite(os.path.join(save_path + '/Fake/', original_filename), img)\n",
    "    \n",
    "    del img\n",
    "    del info\n",
    "    del roi\n",
    "    del classes\n",
    "    del masks\n",
    "\n",
    "    return normality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "362cb89c-e34a-4767-92a6-ac32fba67718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hough_Transform(img, brighted):\n",
    "    gray1 = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray2 = cv2.GaussianBlur(gray1, (9, 9), 2)\n",
    "    \n",
    "    \"\"\"\n",
    "    image, method, dp\n",
    "    4) minDist : 검출된 원들 간의 최소 거리, 검출할 원의 중심 사이의 최소 거리로, 값이 작을수록 원이 더 많이 검출됨\n",
    "    5) param1 : Canny edge 검출기의 상위 임계값\n",
    "    6) param2 : 색상 민감도, 숫자가 작을수록 민감도가 떨어짐\n",
    "    7) minRadius : 검출할 원의 최소 반지름\n",
    "    8) maxRadius : 검출할 원의 최대 반지름\n",
    "    \"\"\"\n",
    "    if brighted:\n",
    "        return cv2.HoughCircles(gray2, cv2.HOUGH_GRADIENT, 1, minDist=25, param1=13, param2=19, minRadius=35, maxRadius=56) #22\n",
    "    else:\n",
    "        return cv2.HoughCircles(gray1, cv2.HOUGH_GRADIENT, 1, 20, param1=62, param2=34, minRadius=10, maxRadius=100)\n",
    "\n",
    "def detection(img, method, brighted, DL):\n",
    "    \n",
    "    if DL:\n",
    "        \"\"\"\n",
    "        딥러닝 알고리즘\n",
    "        \"\"\"\n",
    "    else:\n",
    "        return method(img, brighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "22a31711-5e76-45f5-8e1a-7175f82b9049",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "진행률 : 10/624\n",
      "진행률 : 20/624\n",
      "진행률 : 30/624\n",
      "진행률 : 40/624\n",
      "진행률 : 50/624\n",
      "진행률 : 60/624\n",
      "진행률 : 70/624\n",
      "진행률 : 80/624\n",
      "진행률 : 90/624\n",
      "진행률 : 100/624\n",
      "진행률 : 110/624\n",
      "진행률 : 120/624\n",
      "진행률 : 130/624\n",
      "진행률 : 140/624\n",
      "진행률 : 150/624\n",
      "진행률 : 160/624\n",
      "진행률 : 170/624\n",
      "진행률 : 180/624\n",
      "진행률 : 190/624\n",
      "진행률 : 200/624\n",
      "진행률 : 210/624\n",
      "진행률 : 220/624\n",
      "진행률 : 230/624\n",
      "진행률 : 240/624\n",
      "진행률 : 250/624\n",
      "진행률 : 260/624\n",
      "진행률 : 270/624\n",
      "진행률 : 280/624\n",
      "진행률 : 290/624\n",
      "진행률 : 300/624\n",
      "진행률 : 310/624\n",
      "진행률 : 320/624\n",
      "진행률 : 330/624\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 24\u001b[0m\n\u001b[0;32m     20\u001b[0m [classes, _, masks] \u001b[38;5;241m=\u001b[39m YOLOv8_small_seg\u001b[38;5;241m.\u001b[39mpredict(source\u001b[38;5;241m=\u001b[39mimg, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, show_boxes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, show_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, show_conf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, imgsz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1312\u001b[39m, conf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, retina_masks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     22\u001b[0m img \u001b[38;5;241m=\u001b[39m zoom(img, masks\u001b[38;5;241m=\u001b[39mmasks)\n\u001b[1;32m---> 24\u001b[0m errorRate \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mclassification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msavenable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_timekeeping: \n\u001b[0;32m     27\u001b[0m     keeped \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m timekeeping\n",
      "Cell \u001b[1;32mIn[42], line 41\u001b[0m, in \u001b[0;36mclassification\u001b[1;34m(img, save_info, visualizer)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m         roi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[43mdetection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHough_Transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbrighted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbrightiable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDL\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep_available\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m info\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     43\u001b[0m     info \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39muint16(np\u001b[38;5;241m.\u001b[39maround(info))\n",
      "Cell \u001b[1;32mIn[43], line 25\u001b[0m, in \u001b[0;36mdetection\u001b[1;34m(img, method, brighted, DL)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m    딥러닝 알고리즘\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbrighted\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[43], line 14\u001b[0m, in \u001b[0;36mHough_Transform\u001b[1;34m(img, brighted)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mimage, method, dp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m4) minDist : 검출된 원들 간의 최소 거리, 검출할 원의 중심 사이의 최소 거리로, 값이 작을수록 원이 더 많이 검출됨\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m8) maxRadius : 검출할 원의 최대 반지름\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m brighted:\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHoughCircles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHOUGH_GRADIENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminDist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m13\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m19\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminRadius\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m35\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxRadius\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m56\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#22\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mHoughCircles(gray1, cv2\u001b[38;5;241m.\u001b[39mHOUGH_GRADIENT, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m20\u001b[39m, param1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m62\u001b[39m, param2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m34\u001b[39m, minRadius\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, maxRadius\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "img_loader = None\n",
    "\n",
    "if deep_available:\n",
    "    transform = transforms.ToTensor()\n",
    "    img_loader = Image_Loader(dir_path=path, brighter=brightiable, transformer=transform)\n",
    "else:\n",
    "    img_loader = Image_Loader(dir_path=path, brighter=brightiable)\n",
    "     \n",
    "timekeeping = 0.0\n",
    "keeped_times = 0.0\n",
    "            \n",
    "if is_timekeeping: \n",
    "    timekeeping = time.time() \n",
    "\n",
    "count = [0]\n",
    "itr = 1\n",
    "errorRate =0\n",
    "\n",
    "for img, original_filename in img_loader:\n",
    "    [classes, _, masks] = YOLOv8_small_seg.predict(source=img, save=True, show_boxes=True, show_labels=False, show_conf=False, imgsz=1312, conf=0.6, retina_masks=False)\n",
    "\n",
    "    img = zoom(img, masks=masks)\n",
    "\n",
    "    errorRate += classification(img, save_info=savenable, visualizer=visualable)\n",
    "    \n",
    "    if is_timekeeping: \n",
    "        keeped = time.time() - timekeeping\n",
    "        if visualable:\n",
    "            print(f'처리속도 : {keeped}초\\n')\n",
    "        elif itr <= 25:\n",
    "            keeped_times += keeped\n",
    "    \n",
    "    if visualable:\n",
    "        if not is_timekeeping: \n",
    "            print()\n",
    "    else:\n",
    "        if itr % 10 == 0:\n",
    "            print(f'진행률 : {itr}/{len(img_loader)}')\n",
    "\n",
    "    itr += 1\n",
    "    \n",
    "    if is_timekeeping: \n",
    "        timekeeping = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1a7695-6f1f-435a-a310-a806dbe8e663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불량 탐지율 : 251/252\n",
      "저장된 경로 : c:\\Users\\user\\Detection8\\saved_imgs/3NG\n",
      "평균 처리속도 : 0.4718578815460205초\n",
      "AF 2개 미만 탐지 : 6회\n"
     ]
    }
   ],
   "source": [
    "''' 결과 로그 출력 '''\n",
    "\n",
    "if 'NG' in path:\n",
    "    print(f'불량 탐지율 : {len(img_loader) - errorRate}/{len(img_loader)}')\n",
    "else:\n",
    "    print(f'정상 탐지율 : {errorRate}/{len(img_loader)}')\n",
    "\n",
    "print(f'저장된 경로 : {save_path}')\n",
    "print(f'평균 처리속도 : {keeped_times / 25.0}초')\n",
    "print(f'AF 2개 미만 탐지 : {distance_zero_count}회')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58b95ae-d920-4361-9685-781011919c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.996031746031746"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "251/252"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
